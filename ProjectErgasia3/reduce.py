# -*- coding: utf-8 -*-
"""ProjectErgasia3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h5enQn78s39LY9SVWgS7JsEF_02XMJZ7
"""

import numpy as np
import argparse

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
import subprocess
import sys

def main():
    parser = argparse.ArgumentParser(description='Reduce dimensions of dataset and queryset.')
    parser.add_argument('-d', '--dataset', required=True, help='Path to the dataset file')
    parser.add_argument('-q', '--queryset', required=True, help='Path to the queryset file')
    parser.add_argument('-od', '--output_dataset', required=True, help='Path to the output dataset file')
    parser.add_argument('-oq', '--output_query', required=True, help='Path to the output query file')
    #parser.add_argument('-m', '--method', required=True, help='Train (1) or use (2) model', type=int)

    args = parser.parse_args()

    # Load and preprocess the dataset
    dataset = load_dataset(args.dataset)
    dataset = preprocess_data(dataset)
    query = load_dataset(args.queryset)
    query = preprocess_data(query)
    # print(f"Dataset path: {dataset_path}")

    install_package("scikit-learn")
    install_package("scikit-image")
    install_package("tensorflow")
    
    # Splitting the dataset
    x_train, x_val = train_test_split(dataset, test_size=0.2, random_state=42)

    # Create and train the model
    autoencoder = build_autoencoder()
    history = autoencoder.fit(x_train, x_train, epochs=5, batch_size=8, validation_data=(x_val, x_val))

        #Save the model
    autoencoder.save('mnist_autoencoder.h5')

    encoder = models.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('max_pooling2d').output)

    # After training model we evaluate
    evaluate_model(autoencoder, x_val)

    
    #Exporting files
    #encoded_data = encoder.predict(dataset)
    #rescaled_output = (encoded_data * 255).astype(np.uint8)
    #save_encoded_images(args.output_dataset, rescaled_output)

    encoded_query = encoder.predict(query)
    #encoded_data = predict_in_batches(encoder, dataset)
    
    rescaled_query = (encoded_query * 255).astype(np.uint8)

    save_encoded_images(args.output_query, rescaled_query)
    


    
def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])


def load_dataset(file_path):
    with open(file_path, 'rb') as file:
        # Read the header
        magic_number = int.from_bytes(file.read(4), 'big')
        num_images = int.from_bytes(file.read(4), 'big')
        rows = int.from_bytes(file.read(4), 'big')
        cols = int.from_bytes(file.read(4), 'big')

        # Read the pixel data
        images = np.fromfile(file, dtype=np.uint8)
        images = images.reshape(num_images, rows, cols)

    return images

#data = args.dataset

# Preprocess the data
def preprocess_data(data):
    data = data.astype('float32') / 255.0
    data = np.reshape(data, (len(data), 28, 28, 1))
    return data




#data = preprocess_data(dataset)

# Building the Convolutional Autoencoder

from tensorflow.keras import layers, models

def build_autoencoder():
    input_img = layers.Input(shape=(28, 28, 1))

    # Encoder
    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
    encoded = layers.MaxPooling2D((2, 2), padding='same')(x) # 14x14

    # Decoder
    x = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(encoded)
    x = layers.UpSampling2D((2,2))(x)
    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

    autoencoder = models.Model(input_img, decoded)
    autoencoder.compile(optimizer='Nadam', loss='binary_crossentropy')

    return autoencoder







def predict_in_batches(model, data, batch_size=8):
    num_samples = len(data)
    predictions = []

    for start_idx in range(0, num_samples, batch_size):
        end_idx = min(start_idx + batch_size, num_samples)
        batch_predictions = model.predict(data[start_idx:end_idx])
        predictions.append(batch_predictions)

    return np.concatenate(predictions, axis=0)









from tensorflow.keras.models import Model


from sklearn.metrics import mean_squared_error
from skimage.metrics import structural_similarity as ssim
# import matplotlib.pyplot as plt

def evaluate_model(model, x_val):
    # Predict the reconstructed images
    reconstructed = model.predict(x_val)

    win_size = 5
    channel_axis = -1
    data_range = 1.0 
    # Calculate MSE and SSIM for each image
    ssim_values = [ssim(x_val[i], reconstructed[i], multichannel=False, win_size=win_size, data_range=data_range, channel_axis=channel_axis) for i in range(len(x_val))]

    mse_values = [mean_squared_error(x_val[i].flatten(), reconstructed[i].flatten()) for i in range(len(x_val))]
    #ssim_values = [ssim(x_test[i], reconstructed[i], multichannel=True) for i in range(len(x_test))]

    # Calculate average MSE and SSIM
    avg_mse = np.mean(mse_values)
    avg_ssim = np.mean(ssim_values)

    print(f"Average MSE: {avg_mse}")
    print(f"Average SSIM: {avg_ssim}")





# import matplotlib.pyplot as plt
# # Plot training & validation loss values
# plt.figure(figsize=(8, 4))
# plt.plot(history.history['loss'], label='Training Loss')
# plt.plot(history.history['val_loss'], label='Validation Loss')
# plt.title('Model Loss')
# plt.ylabel('Loss')
# plt.xlabel('Epoch')
# plt.legend(loc='upper right')
# plt.show()



def save_encoded_images(file_path, encoded_images):
    with open(file_path, 'wb') as file:
        # Write the header
        magic_number = 1234  # Or use the original magic number
        file.write(magic_number.to_bytes(4, 'big'))

        num_images = encoded_images.shape[0]
        file.write(num_images.to_bytes(4, 'big'))

        # For encoded images, rows and cols might be different
        rows = encoded_images.shape[1]
        cols = 1 if len(encoded_images.shape) == 2 else encoded_images.shape[2]
        file.write(rows.to_bytes(4, 'big'))
        file.write(cols.to_bytes(4, 'big'))

        # Write the pixel data
        for image in encoded_images:
            file.write(image.astype(np.uint8).tobytes())




"""#Graphs"""

# import matplotlib.pyplot as plt

# # Data
# layers = [1, 2, 4, 6]
# average_mse = [0.0017412564484402537, 0.006280350498855114, 0.012241659685969353,0.025430526584386826]
# average_ssim = [0.9837241172790527, 0.9463238716125488, 0.8872165679931641,0.7521872520446777]

# # Create a figure and a set of subplots
# fig, ax1 = plt.subplots()

# # Plotting the Average MSE
# ax1.set_xlabel('Number of Layers')
# ax1.set_ylabel('Average MSE', color='tab:red')
# ax1.plot(layers, average_mse, color='tab:red', marker='o')
# ax1.tick_params(axis='y', labelcolor='tab:red')

# # Instantiate a second axes that shares the same x-axis
# ax2 = ax1.twinx()
# ax2.set_ylabel('Average SSIM', color='tab:blue')
# ax2.plot(layers, average_ssim, color='tab:blue', marker='o')
# ax2.tick_params(axis='y', labelcolor='tab:blue')

# # Title and show
# plt.title('Autoencoder Performance vs Number of Layers')
# fig.tight_layout()
# plt.show()

# import matplotlib.pyplot as plt

# # Data from the table
# filter_combinations = ['3x3-3x3', '5x5-5x5', '7x7-7x7', '3x3-5x5', '3x3-7x7', '5x5-7x7', '7x7-3x3', '7x7-5x5']
# average_mse = [
#     0.0017412564484402537, 0.001814944902434945, 0.0018681043293327093,
#     0.001578118302859366, 0.0018835437949746847, 0.0017380034551024437,
#     0.0016391354147344828, 0.0018124044872820377
# ]
# average_ssim = [
#     0.9837241172790527, 0.9818357825279236, 0.9826061725616455,
#     0.9850969314575195, 0.9819301962852478, 0.9836792945861816,
#     0.9844864010810852, 0.9823940396308899
# ]

# # Create a figure and a set of subplots
# fig, ax1 = plt.subplots(figsize=(8, 4))

# # Plotting the Average MSE
# ax1.set_xlabel('Filter Combination (Encoder-Decoder)')
# ax1.set_ylabel('Average MSE', color='tab:red')
# ax1.plot(filter_combinations, average_mse, color='tab:red', marker='o', label='Average MSE')
# ax1.tick_params(axis='y', labelcolor='tab:red')
# ax1.set_xticks(range(len(filter_combinations)))
# ax1.set_xticklabels(filter_combinations, rotation=45, ha='right')

# # Instantiate a second axes that shares the same x-axis
# ax2 = ax1.twinx()
# ax2.set_ylabel('Average SSIM', color='tab:blue')
# ax2.plot(filter_combinations, average_ssim, color='tab:blue', marker='o', label='Average SSIM')
# ax2.tick_params(axis='y', labelcolor='tab:blue')

# # Title and show
# plt.title('Autoencoder Performance by Filter Combination')
# fig.tight_layout()  # Adjust layout to prevent overlap
# fig.legend(loc='upper right', bbox_to_anchor=(1, 1))
# plt.show()

# import matplotlib.pyplot as plt

# # Data from the table
# number_of_filters = [8, 16, 32, 64, 128]
# average_mse = [0.002277400577440858, 0.001578118302859366, 0.0012151864357292652,
#                0.0010107859270647168, 0.000879693659953773]

# # Create a plot
# plt.figure(figsize=(8, 4))
# plt.plot(number_of_filters, average_mse, marker='o', color='blue')

# # Add title and labels
# plt.title('Average MSE vs. Number of Filters')
# plt.xlabel('Number of Filters')
# plt.ylabel('Average MSE')

# # Show the plot
# plt.show()

# import matplotlib.pyplot as plt

# # Data from the table
# batch_sizes = [16, 32, 64, 128, 256, 512]
# average_ssim = [0.9948474168777466, 0.9926042556762695, 0.9891611337661743,
#                 0.9866930246353149, 0.9806249141693115, 0.972266435623169]

# # Create a line graph
# plt.figure(figsize=(8, 4))
# plt.plot(batch_sizes, average_ssim, marker='o', color='blue', linestyle='-', linewidth=2)

# # Add titles and labels
# plt.title('Average SSIM vs Batch Sizes')
# plt.xlabel('Batch Size')
# plt.ylabel('Average SSIM')

# # Add grid for better readability
# plt.grid(True)

# # Optionally, set x-axis to logarithmic scale if needed


# # Show the plot
# plt.show()

# import matplotlib.pyplot as plt
# import numpy as np

# # Data from the table
# optimizers = ['Adam', 'SGD', 'RMSProp', 'Adagrad', 'Nadam']
# average_mse = [0.0006247805431485176, 0.003941364586353302, 0.0008280455367639661,
#                0.01079917885363102, 0.0006030414369888604]
# average_ssim = [0.9948474168777466, 0.9625266790390015, 0.9929535984992981,
#                 0.8862730860710144, 0.9948888421058655]

# # Set positions for the bars
# barWidth = 0.4
# r1 = np.arange(len(average_mse))
# r2 = [x + barWidth for x in r1]

# # Create a figure and a set of subplots
# fig, ax1 = plt.subplots(figsize=(8, 4))

# # Plotting the Average MSE
# ax1.set_xlabel('Optimizers')
# ax1.set_ylabel('Average MSE', color='tab:red')
# ax1.bar(r1, average_mse, width=barWidth, color='tab:red', label='Average MSE')
# ax1.tick_params(axis='y', labelcolor='tab:red')
# ax1.set_xticks([r + barWidth / 2 for r in range(len(r1))])
# ax1.set_xticklabels(optimizers)

# # Instantiate a second axes that shares the same x-axis
# ax2 = ax1.twinx()
# ax2.set_ylabel('Average SSIM', color='tab:blue')
# ax2.bar(r2, average_ssim, width=barWidth, color='tab:blue', label='Average SSIM')
# ax2.tick_params(axis='y', labelcolor='tab:blue')

# # Title and show
# plt.title('Performance of Different Optimizers')
# fig.tight_layout()
# plt.show()

# import matplotlib.pyplot as plt

# # Data from the table
# activation_layers = ['ReLU', 'Sigmoid', 'Tanh', 'Leaky ReLU', 'ELU', 'SeLU']
# average_mse = [0.0006030414369888604, 0.0004924534005112946, 0.0005835855263285339,
#                0.0007499769562855363, 0.0006706789135932922, 0.0005904467543587089]

# # Create a line graph for Average MSE
# plt.figure(figsize=(8, 4))
# plt.plot(activation_layers, average_mse, marker='o', color='blue', linestyle='-')

# # Add titles and labels
# plt.title('Average MSE vs Activation Layers')
# plt.xlabel('Activation Layer')
# plt.ylabel('Average MSE')

# # Show the plot
# plt.show()



if __name__ == '__main__':
    main()
